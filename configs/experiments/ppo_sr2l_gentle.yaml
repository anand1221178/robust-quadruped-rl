defaults:
  - /train/sr2l           # Use SR2L training config
  - /env/realant          # Use RealAnt environment

# Experiment settings
experiment:
  description: "Gentle SR2L initialized from fast target walker - Phase 3 refined"
  name: ppo_sr2l_gentle
  tags: 
    - ppo
    - sr2l
    - phase3
    - smoothness
    - gentle
    - initialized

# Domain randomization disabled (Phase 4)
domain_randomization:
  enabled: false

# VERY GENTLE SR2L settings to preserve walking behavior
sr2l:
  enabled: true
  lambda: 0.001                   # Much gentler regularization (was 0.005)
  perturbation_std: 0.01          # Smaller perturbations (was 0.015)
  warmup_steps: 2000000           # 2M step warmup - let it walk first! (was 500k)
  apply_frequency: 4              # Less frequent application (was 2)
  use_huber_loss: true            # Robust loss for outliers
  max_perturbation: 0.03          # Smaller max perturbation (was 0.05)

# Environment settings - Use target walking for fast walking
env:
  name: RealAntMujoco-v0
  use_target_walking: true       # Use target walking for fast movement
  target_distance: 5.0            # Same as the successful model
  
# Longer training for better results (matching successful baseline)
total_timesteps: 15000000        # 15M steps like the successful models

# Initialize from the successful target walking model
pretrained_model: experiments/ppo_target_walking_llsm451b/best_model/best_model.zip

# Enable W&B logging
logging:
  wandb: true
  wandb_project: "robust-quadruped-rl"
  wandb_entity: "anandpatel1221178-university-of-the-witswatersrand"
  verbose: 1

# Save more frequently to track progress
save_freq: 100000
eval_freq: 25000