defaults:
  - /train/base              # Use base PPO training config
  - /env/realant             # Use RealAnt environment

# Experiment settings
experiment:
  description: "PPO + Domain Randomization - Joint dropout & sensor noise robustness"
  name: ppo_dr
  tags: 
    - ppo
    - domain-randomization
    - phase4
    - joint-dropout
    - sensor-noise
    - curriculum

# Enable Domain Randomization
domain_randomization:
  enabled: true
  use_curriculum: true
  
  # Phase 2 settings (first 5M steps): Single joint + mild noise
  phase_2_steps: 5000000
  phase_2_config:
    joint_dropout_prob: 0.2      # 20% episodes have joint failure
    max_dropped_joints: 1        # Single joint dropout
    min_dropped_joints: 1
    sensor_noise_std: 0.02       # Mild sensor noise
    noise_joints_only: true      # Only joint sensors (per proposal)
  
  # Phase 3 settings (5M+ steps): Multiple joints + high noise  
  phase_3_steps: 15000000
  phase_3_config:
    joint_dropout_prob: 0.4      # 40% episodes have joint failures
    max_dropped_joints: 3        # Up to 3 joint dropouts
    min_dropped_joints: 1
    sensor_noise_std: 0.05       # Higher sensor noise
    noise_joints_only: true

# Disable SR2L (this is PPO+DR only)
sr2l:
  enabled: false

# Environment settings - Use target walking (consistent with baseline)
env:
  name: RealAntMujoco-v0
  use_target_walking: true       # Goal-directed navigation
  target_distance: 5.0           # Same as baseline
  use_domain_randomization: true # Enable DR wrapper
  
# Longer training for curriculum (15M steps)
total_timesteps: 15000000        

# Initialize from successful baseline for fair comparison
pretrained_model: experiments/ppo_target_walking_llsm451b/best_model/best_model.zip

# Enable W&B logging
logging:
  wandb: true
  wandb_project: "robust-quadruped-rl"
  wandb_entity: "anandpatel1221178-university-of-the-witswatersrand"
  verbose: 1

# Save frequently to monitor curriculum progress
save_freq: 100000
eval_freq: 25000

# PPO hyperparameters (same as successful baseline)
ppo:
  learning_rate: 0.0003
  n_steps: 2048
  batch_size: 2048
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.0
  vf_coef: 0.5
  max_grad_norm: 0.5