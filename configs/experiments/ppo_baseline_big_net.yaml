defaults:
  - /train/default           # Use default PPO training config
  - /env/realant             # Use RealAnt environment

# Experiment settings
experiment:
  description: "Big Network Baseline - Testing if 128â†’256 hidden units improves performance"
  name: ppo_baseline_big_net
  tags: 
    - ppo
    - baseline
    - big-network
    - 256-hidden
    - architecture-study

# Disable robustness methods (pure baseline)
sr2l:
  enabled: false
  
domain_randomization:
  enabled: false

# Environment settings - Match current baseline exactly
env:
  name: RealAntMujoco-v0
  use_success_reward: true       # Same as current baseline
  use_domain_randomization: false

# Training duration - Same as current baseline for fair comparison
total_timesteps: 10000000        # 10M steps (same as original baseline)

# PPO hyperparameters - IDENTICAL to current baseline
ppo:
  learning_rate: 0.0003          # Same as baseline
  n_steps: 2048                  # Same as baseline  
  batch_size: 2048               # Same as baseline
  n_epochs: 10                   # Same as baseline
  gamma: 0.99                    # Same as baseline
  gae_lambda: 0.95               # Same as baseline
  clip_range: 0.2                # Same as baseline
  ent_coef: 0.0                  # Same as baseline
  vf_coef: 0.5                   # Same as baseline
  max_grad_norm: 0.5             # Same as baseline

# BIGGER Network architecture - 2x hidden units
policy:
  hidden_sizes:
    - 128                        # 2x baseline (was 64)
    - 256                        # 2x baseline (was 128)
  activation: relu               # Same as baseline

# Enable W&B logging
logging:
  wandb: true
  wandb_project: "robust-quadruped-rl"
  wandb_entity: "anandpatel1221178-university-of-the-witswatersrand"
  verbose: 1

# Save settings
save_freq: 100000
eval_freq: 25000

# NO pretrained initialization - train from scratch
# pretrained_model: null