defaults:
  - /train/base              # Use base PPO training config
  - /env/realant             # Use RealAnt environment

# Experiment settings
experiment:
  description: "PPO + Persistent DR - Realistic failure durations (not single timesteps)"
  name: ppo_persistent_dr
  tags: 
    - ppo
    - persistent-dr
    - realistic-failures
    - extended-durations
    - true-adaptation
    - 30M-steps

# Disable standard Domain Randomization (using persistent instead)
domain_randomization:
  enabled: false

# Disable SR2L (this is DR only)
sr2l:
  enabled: false

# Environment settings - ENABLE PERSISTENT DR
env:
  name: RealAntMujoco-v0
  use_success_reward: true       # Match baseline wrapper exactly
  use_domain_randomization: false # Disable standard DR
  
  # ENABLE PERSISTENT DR - Key difference from standard DR
  use_persistent_dr: true         # Enable persistent failures
  persistent_dr:
    # Failure probability and severity
    failure_prob: 0.15           # 15% of episodes have failures
    max_failed_joints: 2         # Up to 2 joints can fail
    
    # Realistic failure durations (in timesteps) - KEY INNOVATION
    duration_probs: [0.4, 0.4, 0.2]  # [short, medium, long]
    short_duration: [50, 200]     # Brief issues (2.5-10 seconds at 20Hz)
    medium_duration: [200, 1000]  # Temporary damage (10-50 seconds)
    # Long = entire episode (permanent for that episode)
    
    # Failure types
    failure_types: ['lock', 'weak', 'erratic']
    failure_type_probs: [0.5, 0.3, 0.2]  # Lock most common (stuck joint)
    
    # Progressive curriculum (like successful DR v2)
    use_curriculum: true
    warmup_steps: 8000000         # 8M steps warmup
    curriculum_steps: 15000000    # 15M gradual increase
  
# Extended training duration like successful DR v2
total_timesteps: 30000000        # 30M steps total

# Initialize from baseline like successful DR approaches
pretrained_model: done/ppo_baseline_ueqbjf2x/best_model/best_model.zip

# PPO hyperparameters - MATCH BASELINE EXACTLY
ppo:
  learning_rate: 0.0003
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.0                  # No entropy bonus
  vf_coef: 0.5
  max_grad_norm: 0.5

# Network architecture (DR doesn't have NaN issues, use ReLU like baseline)
policy:
  hidden_sizes: [64, 128]       # Same as baseline
  activation: relu              # ReLU (DR works fine with this)

# Evaluation settings
eval_freq: 10000                # Frequent evaluation
n_eval_episodes: 5              
save_freq: 100000               # Save every 100k steps

# Logging
logging:
  wandb: true
  wandb_project: "robust-quadruped-rl"
  wandb_name: "ppo_persistent_dr"
  verbose: 1

# Notes:
# - Failures persist for realistic durations (not single timesteps)
# - Short: Component stress (2.5-10 seconds)
# - Medium: Temporary damage (10-50 seconds)  
# - Long: Episode-long (simulates permanent damage for that trial)
# - Should teach true adaptation, not just momentary recovery
# - Compare to: standard DR (single-step), permanent DR (never recovers)