defaults:
  - /train/sr2l              # Use SR2L training config
  - /env/realant             # Use RealAnt environment

# Experiment settings
experiment:
  description: "PPO + SR2L trained from SCRATCH - Fixed gradient flow implementation"
  name: ppo_sr2l_scratch
  tags: 
    - ppo
    - sr2l
    - phase3
    - from-scratch
    - fixed-gradients
    - sensor-noise-robustness

# SR2L configuration - Very gentle to start
sr2l:
  enabled: true
  lambda: 0.0005                  # Even gentler lambda for from-scratch training
  perturbation_std: 0.005         # Very small perturbations initially
  warmup_steps: 1000000           # 1M warmup to learn basic walking first
  apply_frequency: 4              # Apply less frequently
  use_huber_loss: false           # Use standard L2 loss
  max_perturbation: 0.02          # Very small max perturbation

# Environment settings - Use target walking
env:
  name: RealAntMujoco-v0
  use_target_walking: true       # Goal-directed navigation
  target_distance: 5.0           # Same as baseline
  
# NO pretrained model - train from scratch
# pretrained_model: null

# Longer training since from scratch
total_timesteps: 20000000        # 20M steps

# Enable W&B logging
logging:
  wandb: true
  wandb_project: "robust-quadruped-rl"
  wandb_entity: "anandpatel1221178-university-of-the-witswatersrand"
  verbose: 1

# Save frequently to monitor progress
save_freq: 100000
eval_freq: 25000

# PPO hyperparameters (same as successful baseline)
ppo:
  learning_rate: 0.0003
  n_steps: 2048
  batch_size: 2048
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.0
  vf_coef: 0.5
  max_grad_norm: 0.5