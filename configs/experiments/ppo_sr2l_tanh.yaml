defaults:
  - /train/base              # Use base PPO training config
  - /env/realant             # Use RealAnt environment

# Experiment settings
experiment:
  description: "PPO + SR2L with Tanh Activation - Fix for NaN issues"
  name: ppo_sr2l_tanh
  tags: 
    - ppo
    - sr2l
    - tanh-activation
    - nan-fix
    - bounded-actions
    - joint-perturbations
    - 30M-steps

# Enable SR2L (Smooth Regularized RL) with TANH ACTIVATION
sr2l:
  enabled: true
  lambda_smooth: 0.001          # Gentle regularization (like successful DR v2)
  perturbation_std: 0.01        # Small perturbations (sensor noise level)
  max_perturbation: 0.05        # Cap maximum perturbation
  warmup_steps: 8000000         # 8M warmup like successful DR v2
  curriculum_steps: 15000000    # 15M progressive increase like DR v2
  perturb_only_joints: true     # Only perturb joint observations (dims 13-28)
  joint_start_dim: 13
  joint_end_dim: 28
  
  # Use curriculum like successful DR v2
  use_curriculum: true

# Disable Domain Randomization (this is pure SR2L test)
domain_randomization:
  enabled: false

# Environment settings - Match baseline exactly
env:
  name: RealAntMujoco-v0
  use_success_reward: true       # Match baseline wrapper exactly
  use_domain_randomization: false # Disable DR (SR2L only)
  
# Extended training duration like successful DR v2
total_timesteps: 30000000        # 30M steps total

# Train from scratch with tanh to avoid ReLU pretrained conflicts
pretrained_model: null           # Train from scratch

# PPO hyperparameters - MATCH BASELINE EXACTLY
ppo:
  learning_rate: 0.0003
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.0                  # No entropy bonus (matching baseline)
  vf_coef: 0.5
  max_grad_norm: 0.5

# Network architecture - KEY CHANGE: TANH ACTIVATION
policy:
  hidden_sizes: [64, 128]       # Same as baseline
  activation: tanh              # *** TANH ACTIVATION TO BOUND OUTPUTS ***

# Evaluation settings
eval_freq: 10000                # Frequent evaluation to catch issues
n_eval_episodes: 5              
save_freq: 100000               # Save every 100k steps

# Logging
logging:
  wandb: true
  wandb_project: "robust-quadruped-rl"
  wandb_name: "ppo_sr2l_tanh"
  verbose: 1

# Notes:
# - Tanh activation bounds outputs to [-1, 1] matching RealAnt action space
# - Should prevent unbounded actions causing NaN in physics simulation
# - Training from scratch to avoid ReLU pretrained model conflicts
# - If this still produces NaN, SR2L is fundamentally incompatible with locomotion