# CLAUDE.md - Project Memory & Context

## Project Overview
**Research Project**: Robust Quadruped RL with SR2L (Smooth Regularized Reinforcement Learning)
**Objective**: Implement SR2L algorithm for robust quadruped locomotion using PPO and RealAnt simulation

## Current Status (September 2, 2025)
- **Phase**: 3/4 (READY FOR CLEAN RETRAINING - Decontamination Complete!)
- **PROJECT DECONTAMINATED**: Complete cleanup from months of wrong baseline contamination
  - **Correct Baseline**: done/ppo_baseline_ueqbjf2x (SMOOTH WALKING! SECURED! ‚úÖ)
  - **Wrong Baseline**: ppo_target_walking_llsm451b (jittery, erratic behavior ‚ùå ARCHIVED)
  - **Root Cause**: Mixed up models during development - smooth video was from different model
- **Ready for Training**:
  - **Baseline**: done/ppo_baseline_ueqbjf2x (0.216 ¬± 0.003 m/s, smooth walking)
  - **SR2L Config**: ppo_sr2l_corrected.yaml (joint sensor perturbations, Œª=0.002)
  - **DR Config**: ppo_dr_robust.yaml (progressive curriculum, joint failures)
- **DECONTAMINATION COMPLETE**: 
  - **Baseline Secured**: Moved to `done/` folder (permanent safety)
  - **Configs Fixed**: All use correct baseline path + environment compatibility
  - **Experiments Cleaned**: 32 contaminated models archived
  - **Next Step**: Launch parallel SR2L and DR retraining on cluster

## Project Phases
1. ‚úÖ **Phase 1**: Environment Setup - Migrate from Ant-v4 to RealAnt
2. ‚úÖ **Phase 2**: Train baseline that actually walks
3. üîÑ **Phase 3**: Add SR2L to smooth fast walking (in progress)
4. ‚è≥ **Phase 4**: Domain Randomization Implementation
5. ‚è≥ **Phase 5**: Combined DR+SR2L Training
6. ‚è≥ **Final**: Comprehensive evaluation and comparison

## Key Achievements
- **Speed**: Achieved 0.9 m/s walking (target: 1.0+ m/s) vs original 0.084 m/s
- **Success Rate**: 81-89% success with target walking approach
- **Approach**: Goal-directed navigation (A-to-B) worked better than aggressive speed rewards
- **Problem Identified**: Fast walking is jerky/spazzy, needs SR2L smoothing

## Current Models
### Active Models for Ablation Study
1. **archive/experiments/ppo_baseline_ueqbjf2x** - ‚úÖ CORRECT BASELINE (smooth walking!)
2. **experiments/ppo_sr2l_fixed_v2_resume_5pc1hmrr** - ‚úÖ SR2L (20M steps, completed)
3. **experiments/ppo_dr_robust_4hz3h7pq** - ‚úÖ Domain Randomization (completed)
4. **[FUTURE]** - Combined SR2L+DR model (to be trained)

### Discarded/Wrong Models  
- **ppo_target_walking_llsm451b** - WRONG baseline (erratic behavior, caused months of confusion)
- **ppo_fast_walking_71n0huse** - Speed comparison model (archived)
- **ppo_sr2l_gentle_dpmpni64** - Old SR2L attempt (superseded)
- **ppo_sr2l_corrected_nppuk7mn** - Old corrected SR2L (superseded)

### Archived Models (in archive/experiments/)
- 23 failed/old experiments moved to archive during cleanup

## Technical Implementation
### SR2L Algorithm
- **Formula**: L_total = L_PPO + Œª √ó L_smooth
- **L_smooth**: E[||œÄ(s) - œÄ(s + Œ¥)||¬≤] (policy sensitivity to state perturbations)
- **Current Config**: Œª=0.001, warmup=2M steps, 15M total steps

### Environment Setup
- **Base**: RealAnt-v0 (29D observation space)
- **Wrapper**: TargetWalkingWrapper (goal-directed navigation, 5m targets)
- **Normalization**: VecNormalize for observations and rewards

### Evaluation Metrics
1. **Speed**: Average velocity (m/s)
2. **Smoothness**: Action smoothness score = 1/(1 + mean(action_changes))
3. **Robustness**: Success rate under sensor noise (0%, 5%, 10% noise levels)
4. **Stability**: Angular velocity < 2.0 rad/s

## Recent Major Changes

### September 2, 2025 - üßπ COMPLETE PROJECT DECONTAMINATION & CLEANUP üßπ
- **DECONTAMINATION COMPLETE**: Fixed all contamination from wrong baseline model
  - **Baseline Secured**: Moved `ppo_baseline_ueqbjf2x` from archive ‚Üí `done/` (permanent safety)
  - **All Configs Fixed**: Updated to use `done/ppo_baseline_ueqbjf2x/best_model/best_model.zip`
  - **Environment Compatibility**: Fixed critical wrapper mismatch (use_success_reward vs use_target_walking)
  - **Network Architecture**: Ensured all configs match baseline (64‚Üí128 neurons, ReLU)
  - **Experiments Cleaned**: Moved all 10 contaminated experiments to archive (total: 32 archived)
  - **Config Cleanup**: Kept only 2 essential configs, archived 13 old/unused configs

### September 2, 2025 - ‚úÖ READY FOR CLEAN RETRAINING
- **Final Training Setup**:
  - **SR2L Config**: `configs/experiments/ppo_sr2l_corrected.yaml` (joint sensors only, Œª=0.002)
  - **DR Config**: `configs/experiments/ppo_dr_robust.yaml` (progressive curriculum, joint failures)
  - **Baseline**: `done/ppo_baseline_ueqbjf2x` (0.216 ¬± 0.003 m/s, smooth walking)
- **Environment Compatibility Verified**:
  - All configs use `use_success_reward: true` (matches baseline)
  - Network: 64‚Üí128 hidden units, ReLU activation
  - Training ready for parallel cluster execution using existing sbatch scripts
- **Research Impact**: Project back on track with proper foundation for 4-way ablation study

### September 2, 2025 - üéâ BREAKTHROUGH: FOUND THE REAL BASELINE MODEL! üéâ
- **MISSION CRITICAL DISCOVERY**: We were testing the WRONG baseline model for months!
  - **Problem**: Using `ppo_target_walking_llsm451b` (jittery, erratic walking)
  - **Solution**: Found `archive/experiments/ppo_baseline_ueqbjf2x` (smooth, stable walking!)
  - **Discovery Method**: Git archaeology - traced smooth video back to August 21st commit "GOOD BASELINE"
- **Model Comparison Results**:
  - **Wrong Baseline (target_walking)**: Erratic behavior, flipping, random movements
  - **Correct Baseline (ueqbjf2x)**: Smooth forward walking, stable, maintains balance
  - **Environment**: Both use RealAnt + VecNormalize (29D observation space)
- **Impact on Research**:
  - ALL previous evaluations used wrong baseline - results invalid
  - Explains why we couldn't recreate smooth walking behavior
  - Project can now proceed with proper 4-way ablation study
  - Research timeline back on track!
- **Technical Setup for Smooth Walking**:
  - Model: `done/ppo_baseline_ueqbjf2x/best_model/best_model.zip` (SECURED LOCATION)
  - VecNorm: `done/ppo_baseline_ueqbjf2x/vec_normalize.pkl`
  - Environment: `gym.make('RealAntMujoco-v0')` + VecNormalize + SuccessRewardWrapper
  - Configuration: Basic PPO, 64‚Üí128 network, use_success_reward: true

### September 2, 2025 - CRITICAL EVALUATION BUG DISCOVERED AND FIXED
- **Bug Discovery**: Found catastrophic evaluation bug causing false low performance
  - Target positions were incrementing by 0.01m instead of 5m (x=9.50, 9.51, 9.52...)
  - Robot was jittering in place trying to hit micro-targets instead of walking
  - All models showed ~0.02 m/s (1% of target) due to evaluation bug, not training failure
- **Root Cause**: `debug_velocity.py` using wrong wrapper
  - Was using `SuccessRewardWrapper` instead of `TargetWalkingWrapper`
  - Looking for wrong info keys ('current_velocity' vs 'speed')
  - `record_video.py` missing `--target-walking` flag
- **Training Results (BEFORE fix verification)**:
  - **SR2L Resume (20M)**: 0.025 m/s, 84.3% noise retention, 0% falls at most noise levels
  - **DR Robust**: 0.022 m/s, 119.5% noise retention (!), higher speed under noise
  - Both models completed training but need re-evaluation with fixed scripts
- **Key Insight**: Models may actually be performing well but were evaluated incorrectly
  - SR2L shows excellent stability (0% falls) and noise retention
  - DR shows paradoxical improvement under noise (needs investigation)
  - True performance remains to be measured with fixed evaluation

### September 1, 2025 - NOISE STRESS TEST REVEALS SURPRISING RESULTS
- **Noise Stress Test Results**: Comprehensive robustness evaluation completed
  - **Baseline PPO**: 0.138 m/s (71.4% retention), 0% falls, perfect stability
  - **SR2L Fixed**: 0.563 m/s (74.4% retention), 67-100% falls, speed-instability trade-off
- **Key Discovery**: Current SR2L model is a HIGH-SPEED variant, not the slow robust one
  - SR2L shows 4x higher velocity than baseline (0.563 vs 0.138 m/s)
  - Better noise performance retention (74.4% vs 71.4%)
  - But much higher fall rates (67-100% vs 0%)
- **Performance Pattern**: SR2L learned aggressive, fast-but-risky walking strategy
  - Trades stability for speed and noise tolerance
  - Shows robustness to sensor noise but not to environmental challenges
- **Research Implications**: "Sticking to our guns" approach validated
  - Different SR2L models show different robustness-performance profiles
  - Some prioritize speed+noise tolerance, others prioritize stability
  - Demonstrates SR2L's flexibility in learning different robust strategies

### September 1, 2025 - PERFORMANCE-ROBUSTNESS TRADE-OFF DISCOVERED  
- **Previous Finding**: SR2L provides robustness at ~62% performance cost
  - Baseline: 1.31 m/s (fast but fragile) 
  - SR2L: 0.49 m/s (slower but robust to sensor noise)
  - This trade-off is EXPECTED and validates the approach
- **Unexpected Discovery**: Target switching causes backward walking
  - Both baseline and SR2L show -0.8 m/s when target switches
  - Suggests navigation logic needs investigation  
- **Research Stance**: "Sticking to our guns"
  - Performance drop is acceptable for robustness gains
  - Focus on proving robustness value, not just speed
  - Real-world robots need reliability over raw speed

### September 1, 2025 - COMPLETE DR REDESIGN & SR2L SUCCESS
- **SR2L Fixed v2 SUCCESS**: 
  - Joint-only perturbations (dims 13-28) preserving navigation
  - Curriculum learning (Œª: 0‚Üí0.005 over 7M steps)
  - Achieved 0.49 m/s, 73% success rate
  - Currently resuming training to complete 20M steps
- **Domain Randomization Completely Redesigned**:
  - **NEW: RobustDRWrapper** with proper fault modeling
  - **Position control locking**: Joints resist movement (not just torque=0)
  - **Multiple fault types**: Lock (50%), Weak (30%), Delay (20%)
  - **Progressive curriculum**: 2M warmup, 8M progression
  - **Surprise training**: 90% normal, 10% chaos episodes
  - **Interactive testing concepts**: Click-to-break joints, live fault injection
- **Key Implementation Files**:
  - `src/envs/robust_dr_wrapper.py` - Advanced DR implementation
  - `src/agents/ppo_sr2l.py` - Fixed gradient flow, joint-only perturbations
  - `configs/experiments/ppo_dr_robust.yaml` - New DR config with all fixes

### August 29, 2025 - FROM-SCRATCH TRAINING COMPLETE FAILURE
- **SR2L from scratch**: 0.035 m/s (1.8% of target) - Despite gradient fix, still unstable
  - Numerical instability: NaN/Inf in QVEL and QPOS
  - Physics simulation breaks under SR2L regularization
  - Even gentle parameters (Œª=0.0005) cause catastrophic failure
- **DR from scratch**: -0.023 m/s (NEGATIVE velocity - walks backwards!)
  - Can't learn basic forward locomotion with joint dropouts
  - Even gentle curriculum (15% dropout) prevents learning
- **Key Discovery**: Pretrained initialization is ESSENTIAL
  - Target walking is too hard to learn from scratch with perturbations
  - Need stable walking policy first, then add robustness
  - Both SR2L and DR fail without good initialization

### August 28, 2025 - CRITICAL SR2L BUG FOUND AND FIXED
- **Major Discovery**: Found critical gradient flow bug in SR2L implementation
- **Bug Location**: `src/agents/ppo_sr2l.py` line 62
  - Original actions computed with `torch.no_grad()` - detached from computational graph
  - SR2L loss couldn't properly backpropagate through original actions
  - Caused one-sided regularization and policy instability
- **Fix Applied**: 
  - Removed `torch.no_grad()` wrapper from original action computation
  - Added `noise.detach()` to prevent gradients through random perturbations
  - Now both original and perturbed actions have proper gradients for bidirectional regularization
- **New Training Strategy**: `configs/experiments/ppo_sr2l_scratch.yaml`
  - Train from SCRATCH (no pretrained model conflicts)
  - Very gentle parameters: Œª=0.0005, std=0.005, max_perturb=0.02
  - 1M step warmup before SR2L kicks in, 20M total training
  - Still uses target walking for proper goal-directed locomotion
- **Why All Previous SR2L Failed**: 
  - Gradient flow bug prevented proper regularization
  - Pretrained model weights conflicted with SR2L objectives
  - Too aggressive regularization parameters
  - All attempts (gentle, aggressive, motor, observation) failed due to same bug

### August 23, 2025 - TRUE SR2L Motor Perturbation FAILURE
- **Final Fix Applied**: Now perturbing ACTIONS (motor outputs) instead of observations
- **Implementation**: Simulates 5-10% motor degradation as per research proposal
- **Results**: STILL CATASTROPHIC FAILURE
  - **Speed**: 0.25 ¬± 0.01 m/s (only 19% of baseline's 1.31 m/s) 
  - **Consistency**: Very consistent failure (low std dev)
- **Conclusion**: SR2L fundamentally breaks target walking, regardless of implementation
- **All 4 SR2L attempts failed**: Aggressive, Gentle, Observation-perturbed, Action-perturbed

### August 23, 2025 - Final Gentle SR2L Results
- **Velocity (Corrected Test)**: 
  - **Baseline**: 1.31 ¬± 0.05 m/s (very consistent, 131% of target)
  - **Gentle SR2L**: 1.24 ¬± 0.16 m/s (maintains 95% speed, more variable, 124% of target)
- **Smoothness**: 30% improvement in mean action changes (1.014 ‚Üí 0.711)
- **Robustness Issues**: Still shows 8-30% worse success rates under noise
- **Performance**: Better than aggressive SR2L but still worse than baseline
- **Status**: Gentle SR2L partially successful - good speed + smoothness, poor robustness

## Recent Major Changes

### August 23, 2025 - Directory Cleanup
- Cleaned up project directory, moved 23 old experiments to archive
- Kept only 2 essential models and 4 working configs
- Freed ~1.2GB space, organized archive structure
- Preserved all work in organized archive/ folders
- Added archive/ to .gitignore to prevent large file uploads

### August 23, 2025 - SR2L Fix Attempt
- **Problem**: Original SR2L failed catastrophically (81% ‚Üí 26% success)
- **Solution**: Created ppo_sr2l_gentle with:
  - Initialization from successful target walker
  - Much gentler regularization (Œª=0.001 vs 0.005)
  - Longer warmup period (2M steps)
  - 15M total training steps
- **Status**: Currently training

### Recent Training Results
- **Target Walking**: 0.896 ¬± 0.029 m/s (very consistent)
- **Fast Walking**: 0.748 ¬± 0.373 m/s (more variable)
- **Winner**: Target walking approach for consistent speed

## Key Files & Scripts
### Essential Scripts (scripts/) - **NEEDS UPDATE WITH CORRECT BASELINE**
- `debug_velocity.py` - Test walking speed of models (**UPDATE NEEDED**)
- `evaluate_sr2l.py` - Compare PPO vs SR2L with noise testing (**UPDATE NEEDED**)
- `record_video.py` - Create clean videos of models (**UPDATE NEEDED**)  
- `noise_stress_test.py` - Comprehensive robustness evaluation (**UPDATE NEEDED**)
- `compare_models.py` - Side-by-side model comparison (**UPDATE NEEDED**)
- `test_real_baseline.py` - ‚úÖ **WORKING** script that found correct baseline

### Essential Configs (configs/experiments/)
- `ppo_baseline.yaml` - Reference config for working baseline (**IDENTIFY CORRECT ONE**)
- `ppo_sr2l_fixed_v2_resume.yaml` - SR2L resume config ‚úÖ
- `ppo_dr_robust.yaml` - Domain randomization config ‚úÖ

### **CRITICAL ACTION ITEM**: All evaluation scripts must be updated to use:
- **Baseline Model**: `archive/experiments/ppo_baseline_ueqbjf2x/best_model/best_model.zip`
- **Baseline VecNorm**: `archive/experiments/ppo_baseline_ueqbjf2x/vec_normalize.pkl`

### Core Implementation (src/)
- `train.py` - Main training script with config system
- `agents/ppo_sr2l.py` - Custom PPO+SR2L implementation
- `envs/target_walking_wrapper.py` - Goal-directed navigation
- `envs/success_reward_wrapper.py` - Speed-focused rewards

## SR2L FUNDAMENTAL FIX - ACTION PERTURBATION
**CRITICAL MISUNDERSTANDING FIXED**: SR2L should perturb ACTIONS (motor outputs), not observations!

**Research Proposal Intent**:
- SR2L: Handle **degrading/failing motors** (motors not producing expected torque)
- Domain Randomization: Handle **locked/failed joints** (complete joint failure)

**What We Were Doing Wrong**:
- ‚ùå Perturbing sensor observations (confusing what robot sees)
- ‚ùå Breaking the navigation and balance systems

**Correct Implementation (NOW FIXED)**:
- ‚úÖ Perturb ACTIONS (motor commands) to simulate worn/degrading motors
- ‚úÖ Robot learns policies robust to imperfect motor execution
- ‚úÖ Simulates realistic motor wear: 5-10% torque degradation

## Research Proposal: ABLATION STUDY (CORRECTED)
**Goal**: Compare robustness approaches for quadruped locomotion

**Ablation Components** (CORRECTED with proper baseline):
1. **PPO (Baseline)**: archive/experiments/ppo_baseline_ueqbjf2x - SMOOTH WALKING ‚úÖ
2. **PPO + SR2L**: experiments/ppo_sr2l_fixed_v2_resume_5pc1hmrr - 20M steps ‚úÖ  
3. **PPO + DR**: experiments/ppo_dr_robust_4hz3h7pq - Robust fault modeling ‚úÖ
4. **PPO + SR2L + DR**: To be implemented (Phase 5) ‚è≥

**CRITICAL NOTE**: Previous ablation attempts used wrong baseline model, invalidating all results.
New evaluation will use correct smooth-walking baseline for proper comparison.

**CRITICAL RESEARCH PROPOSAL ANALYSIS**:
**SR2L Purpose (from proposal)**: Handle **sensor noise** and **noisy proprioceptive signals**
- This means **observation perturbation** was CORRECT all along!
- Motor perturbation was misinterpretation of proposal intent

**Research Proposal Scope**:
- **SR2L**: Sensor noise robustness (observation perturbations)
- **Domain Randomization**: Actuator failures (joint lock/dropout)  
- **Target Walking**: Valid training method (forces real locomotion vs lazy strategies)

**Best SR2L Model**: Gentle SR2L (1.24 m/s, 95% baseline speed) - RETRAINING
- Uses observation perturbations (correct for sensor noise per proposal)
- Config and implementation verified for retraining

## Demo & Visualization Tools

### Live Performance Monitor
- **Script**: `scripts/live_performance.py`
- **Features**: Real-time graphs of velocity, distance, rewards, motor commands
- **Usage**: `python scripts/live_performance.py model_path --steps 1000`

### Model Comparison Tool
- **Script**: `scripts/compare_models.py`  
- **Features**: Side-by-side comparison, noise testing, performance plots
- **Usage**: `python scripts/compare_models.py model1 model2 --episodes 5 --noise`

### Real-time Visualizer (Advanced)
- **Script**: `scripts/realtime_visualizer.py`
- **Features**: Multi-panel dashboard with joint angles, smoothness metrics
- **Usage**: `python scripts/realtime_visualizer.py model_path`

## Future Demo & Visualization Ideas

### Visual Demonstrations
- **Robot Survivor Challenge**: Side-by-side noise resistance test
- **Before vs After Comparison**: Show progression from failures to success
- **Learning Curve Story**: Plot all attempts with breakthrough annotations

### Interactive Tools to Build
- **Stress Test Simulator**: CLI tool for noise testing
- **Robustness Playground**: GUI with sliders for real-time fault injection
- **Robot Autopsy Mode**: Frame-by-frame failure analysis
- **Perturbation Sensitivity Heatmap**: Visualize which joints are most critical

### Presentation Concepts
- **Live Demo Format**: Audience calls out noise levels in real-time
- **Interactive Betting**: Audience predicts which model survives
- **Technical Deep-dive**: Show internal states during failures
- **Robot Hospital Records**: "Medical files" for each model's issues

### Quick Wins We Could Build Now
1. **Noise stress test script** - Test robustness under increasing noise
2. **Success rate evaluator** - Count target reaches over many episodes
3. **Smoothness analyzer** - Quantify action jerkiness
4. **Robot fails compilation** - Funny video of worst performances

## Supervisor Communication
- Evaluation metrics are comprehensive (speed, smoothness, robustness)
- Have quantitative ways to measure smoothness improvements
- Initial results show SR2L potential but need to fix performance degradation

---
*This file is updated after every significant change or conversation to maintain project context and memory.*